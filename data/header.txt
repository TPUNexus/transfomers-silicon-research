# Transformer Models Silicon Research

> **Research and Materials on Hardware implementation of Transformer Models**

<!-- <p align="center">
  <img src="https://img.shields.io/badge/-WIP-ff69b4?style=flat-square"/>
</p>
<p align="center">
  <img src="https://img.shields.io/badge/Progress-%2599-ef6c00?labelColor=1565c0&style=flat-square"/>
</p> -->

## How to Contribute

**You can add new papers via pull requests, Please check `data/papers.yaml` and if your paper is not in list, add entity at the last item and create pull request.**

## Transformer and BERT Model

* BERT is a method of **pre-training language representations**, meaning that we **train a general-purpose *language understanding model*** on a large text corpus (like Wikipedia) and then use that model for downstream NLP tasks.

* BERT was created and **published in 2018 by Jacob Devlin and his colleagues from Google**. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks.

<p align="center">
  <img src="./data/img/BERT-ARCH.png" width='480' />
</p>

* **BERT is a Transformer-based model.**
    * The architecture of BERT is similar to the original Transformer model, except that BERT has two separate Transformer models: one for the left-to-right direction (the “encoder”) and one for the right-to-left direction (the “encoder”).
    * The output of each model is the hidden state output by the final Transformer layer. The two models are pre-trained jointly on a large corpus of unlabeled text. The pre-training task is a simple and straightforward masked language modeling objective.
    * The pre-trained BERT model can then be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.

---
